---
title: "DMAOG performance evaluation"
author: "Herminio García-González"
date: "2024-10-09"
output: 
  html_document: default
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(summarytools)
library(ggplot2)
library(FSA)
```

## Loading the dataset

Firstly we have to load the dataset from Github from the available project repository:

```{r}
dataset <- read.csv("C:\\Users\\Herminio\\Desktop\\ExamplesDMAOGPaper\\StatisticalAnalysis\\dmaogEvaluationDataset.csv")[, 2:4]
```

## Descriptive statistics

We obtain the descriptive statistics for each of the tools that we have tested. We include in this analysis the mean, the median, the standard deviation the minimum and the maximum:

```{r}
stby(dataset, dataset$Software, descr, round.digits=5, stats=c("mean", "med", "sd", "min", "max"))
```

## Testing normality

In order to see if we can apply a One-Way ANOVA over the results we must ensure firstly that each of the data series are following a normal distribution. For this we will test again for each of the series that is combination of the tool and the tested method. 

Results for "All items" methods:

```{r}
by(dataset$All_items, dataset$Software, shapiro.test)
```

Results for "Search by fields" methods:
```{r}
by(dataset$Search_by_fields, dataset$Software, shapiro.test)
```

Looking to the results we can see that the the null hypothesis (the sample is following a normal distribution) is rejected in most of the cases, meaning that they those samples are not following a normal distribution and thereby we cannot apply a One-Way ANOVA to them as this is one of the assumptions of this test.

## Testing for differences of means
After the obtained results on normality distribution we can opt for a non-parametric test for difference of means. In this case we will use the Kruskal-Wallis test in which the null hypothesis assumes that the distributions have no differences.

Results for "All items" method:

```{r}
kruskal.test(All_items ~ Software, data = dataset)
```

Results for "Search by fields" method:

```{r}
kruskal.test(Search_by_fields ~ Software, data = dataset)
```

In the light of these results we can see that in both cases the null hypothesis is rejected and the significance of this difference can be classified as statistically highly significant with the significance level  p < 0.001.

## Testing differences between groups
The Kruskal-Wallis tests tells us that there are differences between the four given distributions, however with it we cannot exactly know between which groups are those differences. For this purpose we need to run a post hoc analysis. In this case we will use the Dunn's Test using the Benjamini-Hochberg procedure. 

Results for "All items" method:



```{r}
dunnTest(All_items ~ Software, data = dataset, method = "bh")
```

In the case of the "All items" method we can see that there are very significant differences (p < 0.001) between:

* DMAOG and LDflex
* DMAOG and RDF4J-Beans
* LDflex and LDkit
* LDkit and RDF4J-Beans
* LDflex and Walder
* RDF4J-Beans and Walder
* DMAOG and LDO
* LDflex and LDO
* LDkit and LDO
* LDKit and Walder

There are also significant differences (p < 0.05) between: 

* LDflex and RDF4J-Beans
* LDO and RDF4J-Beans
* LDO and Walder
* DMAOG and Walder
* DMAOG and LDKit

Interestingly, there are no significant differences between DMAOG and LDkit. 

In addition we can calculate the effect size to see how strong is the seen relation:

```{r, echo=FALSE}
library(rstatix)
```

```{r}
d_all_items <- dunn_test(All_items ~ Software, data = dataset, p.adjust.method = "BH")
d_all_items$r <- d_all_items$statistic / sqrt(d_all_items$n1+d_all_items$n2)
d_all_items[, c(1,2,3,10)]
```

As we can see from the previous results the effect size is large (r > 0.5) as per Cohen suggested values of effect size for r. Therefore, we can ensure that:

* DMAOG is faster than LDflex, RDF4J-Beans and LDO
* LDkit is faster than LDflex and RDF4J-Beans, LDO and Walder
* LDO is faster than LDflex
* Walder is faster than RDF4J-Beans
* LDflex is faster than Walder

However, we cannot ensure with the same certainty, due to medium (r <= 0.5 and r > 0.3) or small effect size (r <= 0.3), that: 

* Walder is faster than LDO
* RDF4J-Beans is faster than LDflex
* DMAOG is faster than Walder
* LDO is faster than RDF4J-Beans
* LDkit is faster than DMAOG


Results for "Search by fields" method:

```{r}
dunnTest(Search_by_fields ~ Software, data = dataset, method = "bh")
```

In this case there are very significant differences (p < 0.001) between:
  
* DMAOG and LDflex 
* DMAOG and RDF4J-Beans
* LDkit and RDF4J-Beans
* RDF4J-Beans and Walder
* LDflex and Walder
* LDflex and LDkit
* LDO and Walder
* LDO and RDF4J-Beans
* DMAOG and LDkit
* LDkit and LDO
  
There are significant differences (p < 0.05) between:

* LDflex and RDF4J-Beans
* DMAOG and Walder
* LDflex and LDO
* DMAOG and LDO

There are no significant differences:

* LDkit and Walder
  
Calculating the effect size:

```{r}
d_search_by_fields <- dunn_test(Search_by_fields ~ Software, data = dataset, p.adjust.method = "BH")
d_search_by_fields$r <- d_search_by_fields$statistic / sqrt(d_search_by_fields$n1+d_search_by_fields$n2)
d_search_by_fields[, c(1,2,3,10)]
```

We can see that pairing the previously obtained significance values there are large effect sizes (r > 0.5) between:
  
* DMAOG and LDflex (being DMAOG faster)
* DMAOG and RDF4J-Beans (DMAOG is faster)
* RDF4J-Beans and Walder (Walder is faster)
* LDflex and Walder (Walder is faster)
* LDkit and RDF4J-Beans (LDkit is faster)
* LDflex and LDkit (LDkit is faster)
* LDO and RDF4J-Beans (LDO is faster)
* LDO and Walder (walder is faster)
* DMAOG and LDkit (LDkit is faster)
* LDkit and LDO (LDkit is faster)
  
We see low or medium effect size effects (r <= 0.5) between:

* LDflex and RDF4J-Beans (being LDflex faster)
* DMAOG and Walder (being in this case Walder faster).
* LDflex and LDO (LDO is faster)
* DMAOG and LDO (DMAOG is faster)


So we can corroborate that the previously observed differences and performance classification are statistically valid, so we can classify their performance depending on the used method:

All items: LDkit > DMAOG > Walder > LDO > RDF4J-Beans > LDflex
Search by fields: LDkit/Walder > DMAOG > LDO > LDflex > RDF4J-Beans

# Visualising the differences
We can represent these differences graphically by means of a violin plot which will help the interpretation of these results. However, as the values can vary greatly among the different tools it is necessary to transform the coordinates so that the results can be interpreted more easily.

Violin plot representation for the All items method where the y axis has been transformed based on a log10 scale:
```{r}
dataset %>%
    ggplot(aes(fill=Software, y=All_items, x=Software)) +
        geom_violin(trim=FALSE, scale="width") + 
        coord_trans(y="log10") +
        theme_minimal() + 
        ylab("Elapsed time (ms)") +
        theme(legend.position="right", text = element_text(size=15))
```

Violin plot representation for the Search by fields method where the y axis has been transformed based on a log10 scale:
```{r}
dataset %>%
    ggplot(aes(fill=Software, y=Search_by_fields, x=Software)) +
        geom_violin(trim=FALSE, scale="width") + 
        coord_trans(y="log10") +
        theme_minimal() + 
        ylab("Elapsed time (ms)") +
        theme(legend.position="right", text = element_text(size=15))
```